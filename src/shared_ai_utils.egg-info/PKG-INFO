Metadata-Version: 2.4
Name: shared-ai-utils
Version: 0.1.0
Summary: Shared utilities for AI-assisted development tools: LLM providers, configuration, assessment, patterns, and CLI frameworks
Author: Shared AI Utils Contributors
License: MIT
Project-URL: Homepage, https://github.com/doronpers/shared-ai-utils
Project-URL: Repository, https://github.com/doronpers/shared-ai-utils
Project-URL: Issues, https://github.com/doronpers/shared-ai-utils/issues
Keywords: ai,llm,configuration,assessment,patterns,cli,fastapi,utilities,shared
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: pydantic>=2.0.0
Requires-Dist: pydantic-settings>=2.1.0
Requires-Dist: pyyaml>=6.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: click>=8.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: fastapi>=0.100.0
Requires-Dist: uvicorn>=0.23.0
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.18.0; extra == "anthropic"
Provides-Extra: openai
Requires-Dist: openai>=1.0.0; extra == "openai"
Provides-Extra: gemini
Requires-Dist: google-generativeai>=0.3.0; extra == "gemini"
Provides-Extra: memu
Requires-Dist: memu-py>=0.1.0; extra == "memu"
Provides-Extra: llm
Requires-Dist: anthropic>=0.18.0; extra == "llm"
Requires-Dist: openai>=1.0.0; extra == "llm"
Requires-Dist: google-generativeai>=0.3.0; extra == "llm"
Provides-Extra: all
Requires-Dist: anthropic>=0.18.0; extra == "all"
Requires-Dist: openai>=1.0.0; extra == "all"
Requires-Dist: google-generativeai>=0.3.0; extra == "all"
Requires-Dist: memu-py>=0.1.0; extra == "all"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"

# Shared AI Utils

Shared utilities for AI-assisted development tools. This package provides reusable components extracted and consolidated from multiple repositories to eliminate duplication and ensure consistency across projects.

## Installation

```bash
# Basic installation
pip install shared-ai-utils

# With LLM providers (Anthropic, OpenAI, Gemini)
pip install shared-ai-utils[llm]

# With MemU integration for semantic search
pip install shared-ai-utils[memu]

# All optional dependencies
pip install shared-ai-utils[all]

# Development dependencies
pip install shared-ai-utils[dev]
```

## Components

### LLM Providers

Unified async-first LLM provider interface with automatic fallback and token tracking.

**Features:**
- Async-first API with synchronous wrappers
- Automatic fallback between providers
- Token usage tracking
- Structured output support (JSON schema)
- Streaming support
- Provider availability detection

**Example:**
```python
from shared_ai_utils.llm import LLMManager

manager = LLMManager(preferred_provider="anthropic")
response = await manager.generate(
    system_prompt="You are a helpful assistant.",
    user_prompt="Explain async/await in Python",
    max_tokens=500
)
print(f"Response: {response.text}")
print(f"Tokens used: {response.tokens_used}")
```

**Available Providers:**
- `AnthropicProvider` - Claude models
- `OpenAIProvider` - GPT models
- `GeminiProvider` - Google Gemini models
- `HTTPProvider` - Custom HTTP endpoints

### Configuration System

Type-safe configuration with YAML support, environment variables, and preset management.

**Features:**
- Pydantic `BaseSettings` for type safety and validation
- YAML config file support with auto-loading
- Environment variable auto-loading (`.env` files)
- Dot-notation key access
- Preset system (8 predefined + custom presets)

**Example:**
```python
from shared_ai_utils.config import ConfigBase, ConfigManager, PresetManager

class MyConfig(ConfigBase):
    api_key: str
    debug: bool = False
    max_retries: int = 3

# Load from YAML or environment
config_manager = ConfigManager(config_path="~/.config/myapp/config.yaml")
config = config_manager.load(MyConfig)

# Use presets
preset = PresetManager.get_preset("development")
config_manager.apply_preset(preset)
```

**Predefined Presets:**
- `quick_test` - Minimal config for quick testing
- `development` - Development environment settings
- `testing` - Test environment configuration
- `staging` - Staging environment settings
- `production` - Production-ready configuration
- `high_performance` - Optimized for performance
- `low_resource` - Minimal resource usage
- `ml_development` - ML/AI development settings

### Assessment Engine

Evidence-based multi-path assessment system for explainable evaluation.

**Features:**
- Multi-path evaluation (Technical, Design, Collaboration, Problem-Solving, Communication)
- Evidence-based scoring with confidence levels
- Natural language explanations
- Extensible framework for ML integration
- Micro-motive tracking

**Example:**
```python
from shared_ai_utils.assessment import AssessmentEngine, AssessmentInput

engine = AssessmentEngine(enable_explanations=True)
input_data = AssessmentInput(
    candidate_id="dev-123",
    code_samples=["..."],
    test_results={...},
    collaboration_notes=["..."]
)
result = await engine.assess(input_data)

print(f"Overall Score: {result.overall_score}")
print(f"Confidence: {result.confidence}")
for path_score in result.path_scores:
    print(f"{path_score.path}: {path_score.score}")
```

### FastAPI Utilities

Production-ready FastAPI middleware and utilities for consistent API patterns.

**Features:**
- Request ID middleware for request tracing
- CORS validation with production safety checks
- Standardized error handling with custom error codes
- Health check endpoint utilities
- Request/response models

**Example:**
```python
from fastapi import FastAPI
from shared_ai_utils.api import (
    RequestIDMiddleware,
    create_cors_middleware,
    create_health_router,
    create_error_response
)
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

# Add request ID middleware
app.add_middleware(RequestIDMiddleware)

# Configure CORS
cors_config = create_cors_middleware(
    allowed_origins=["https://example.com"],
    app_env="production"
)
app.add_middleware(CORSMiddleware, **cors_config)

# Add health check
health_router = create_health_router(version="1.0.0")
app.include_router(health_router)

# Use standardized errors
@app.get("/api/data")
async def get_data():
    try:
        # ... your logic
        pass
    except ValueError as e:
        raise create_error_response(
            error_code="INVALID_INPUT",
            message=str(e),
            status_code=400
        )
```

### Pattern Management

Pattern library system with versioning, effectiveness tracking, and semantic search.

**Features:**
- CRUD operations for patterns
- Pattern versioning and archiving
- Effectiveness tracking
- MemU integration for semantic search
- Pattern recommendations

**Example:**
```python
from shared_ai_utils.patterns import PatternManager

manager = PatternManager(pattern_library_path="patterns.json")

# Add a pattern
pattern = manager.add_pattern(
    name="async-error-handling",
    description="Proper async error handling pattern",
    good_example="async def fetch_data(): ...",
    bad_example="def fetch_data(): ...",
    tags=["async", "error-handling", "python"]
)

# Search patterns
suggestions = manager.suggest_patterns("error handling in async code")
for pattern in suggestions:
    print(f"{pattern['name']}: {pattern['description']}")
```

### CLI Framework

Rich terminal utilities and interactive wizards for command-line applications.

**Features:**
- Rich table formatting with colors
- Interactive setup wizards
- Command structure helpers
- Output formatting utilities
- Progress indicators

**Example:**
```python
from shared_ai_utils.cli import print_table, print_success, SetupWizard

# Print formatted table
data = [
    {"name": "Alice", "score": 95},
    {"name": "Bob", "score": 87}
]
print_table(data, title="Assessment Results")

# Interactive wizard
wizard = SetupWizard()
wizard.add_step("api_key", "Enter your API key", required=True)
wizard.add_step("debug", "Enable debug mode?", input_type="boolean")
results = wizard.run()
```

## API Reference

### LLM Module

- `LLMProvider` - Abstract base class for LLM providers
- `LLMManager` - Manager with fallback logic
- `LLMResponse` - Response dataclass with token tracking
- `AnthropicProvider`, `OpenAIProvider`, `GeminiProvider`, `HTTPProvider` - Concrete providers

### Config Module

- `ConfigBase` - Base class for Pydantic-based configuration
- `ConfigManager` - YAML config file manager
- `PresetManager` - Preset management system

### Assessment Module

- `AssessmentEngine` - Main assessment engine
- `AssessmentInput` - Input model
- `AssessmentResult` - Result model with scores and explanations
- `Evidence`, `MicroMotive`, `PathScore` - Supporting models

### API Module

- `RequestIDMiddleware` - FastAPI middleware for request IDs
- `create_cors_middleware()` - CORS configuration factory
- `create_health_router()` - Health check router factory
- `ErrorCode` - Standard error codes enum
- `ErrorResponse` - Standardized error response model
- `create_error_response()` - Error response factory

### Patterns Module

- `PatternManager` - Pattern library manager
- `PatternMemory` - MemU integration for semantic search

### CLI Module

- `BaseCLI` - Base class for Click-based CLIs
- `print_table()`, `print_success()`, `print_error()` - Rich output utilities
- `Wizard`, `SetupWizard` - Interactive wizard framework

## Testing

The package includes comprehensive unit and integration tests:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=shared_ai_utils --cov-report=html

# Run specific test suite
pytest tests/unit/
pytest tests/integration/
```

## Contributing

When adding new components:

1. Follow the existing module structure
2. Add comprehensive type hints
3. Include docstrings for all public APIs
4. Write unit tests with >80% coverage
5. Update this README with examples

## Dependencies

**Core:**
- `pydantic>=2.0.0` - Type validation
- `pydantic-settings>=2.1.0` - Settings management
- `pyyaml>=6.0.0` - YAML support
- `python-dotenv>=1.0.0` - Environment variable loading
- `click>=8.0.0` - CLI framework
- `rich>=13.0.0` - Rich terminal output
- `httpx>=0.24.0` - HTTP client
- `fastapi>=0.100.0` - Web framework
- `uvicorn>=0.23.0` - ASGI server

**Optional:**
- `anthropic>=0.18.0` - Anthropic API
- `openai>=1.0.0` - OpenAI API
- `google-generativeai>=0.3.0` - Google Gemini API
- `memu-py>=0.1.0` - MemU semantic search

## License

MIT License

## Related Projects

This package consolidates functionality from:
- `feedback-loop` - Pattern management and MemU integration
- `council-ai` - LLM provider abstraction and CLI framework
- `sono-eval` - Assessment engine and FastAPI utilities
- `tex-assist-coding` - Documentation patterns
